{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3) Text Classification\n",
    "\n",
    "In this part, we will train a text classifier from the features extracted in Part 2. The goal is to learn a model that can predict the product category from a review's text.\n",
    "\n",
    "To this end, we will extend the pipeline from Part 2 such that a **LinearSVC** classifier is trained. Since we are dealing with multi-class problems, we have to make sure to put a strategy in place that allows binary classifiers to be applicable. Apply vector length normalization before feeding the feature vectors into the classifier, using Normalizer with L2 norm.\n",
    "\n",
    "Using machine learning experiment design to investigate the effects of parameter settings with the help of the functions provided by Spark:\n",
    "\n",
    "- Split the review data into training, validation, and test set\n",
    "- Make experiments reproducible\n",
    "- Use a grid search for parameter optimization:\n",
    "    - Compare different LinearSVC settings by varying the regularization parameter, standardization of training features and maximum number of iterations\n",
    "- Use the MulticlassClassificationEvaluator to estimate performance of trained classifiers on the test set, using F1 measure as criterion.\n",
    "\n",
    "*Commands used in terminal in order to execute this notebook:*\n",
    "- **jupyter nbconvert Part3.ipynb --to script** *(for converting jupyter notebook file to python file in order to execute it via spark-submit)*\n",
    "- **spark-submit --executor-memory 8G --num-executors 4 --total-executor-cores 16 --conf spark.ui.port=5051 Part3.py**\n",
    "\n",
    "For this part, we will first set the configurations and initialize the session and context which we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName(\"Part 3) Text Classification\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will read the data as a json file and select *category* and *reviewText* for further analyses. We will also load the PipelineModel we created in the part 2 of this assignment and save it into a variable called `pipeline_load`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# Read the data as json file and select category and reviewText\n",
    "data = spark.read.json(\"hdfs:///user/data/reviews_devset.json\").select('category', 'reviewText')\n",
    "# Load PipelineModel created in part 2 of the assignment\n",
    "pipeline_load =  PipelineModel.load('/user/Solution/pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make experiments reproducible we will set a seed random number `rnd = 1234` which we will use in spliting.\n",
    "\n",
    "We will the split the dataset into three different train, validation and test sets. \n",
    "- 80% will be split to training set and 20% to test set. The 80% of the training set we will further split it down into 80% for training and 20% for validation set. That will leave us with 64% of the entire dataset for training, 16% for validation and 20% for testing.\n",
    "\n",
    "*Note*: we will use only 15000 random rows from the dataset because otherwise it is taking too much time to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for reproducability\n",
    "rnd = 1234\n",
    "# spliting the data into train, test and validate sets -> 64% for training, 16% for validation and 20% for testing\n",
    "# Note: we will be using only 15000 random rows from the dataset because of the time it takes to train the classifier\n",
    "train, validate, test = spark.createDataFrame(data.rdd.takeSample(False, 15000, seed=rnd)).randomSplit([0.64, 0.16, 0.2], seed=rnd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will normalize the *tfidf* features extracted in the part 2 of the assignment using *L2 Normalizer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# applying L2 Normalizer to the tfidf features extracted\n",
    "normalizer = Normalizer(inputCol=\"tfidf\", outputCol=\"features\", p=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will initialize the `LinearSVC` classifier that we are going to use. \n",
    "In order to handle multi-class problems, we will use `OneVsRest` classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "\n",
    "# set the classifier to SVC\n",
    "classifier = LinearSVC()\n",
    "# use OneVsRest classifier in order to handle multi class binary\n",
    "ml_classifier = OneVsRest(classifier=classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extend the PipelineModel created in part 2 by adding normalizer and ml_classifier to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Extend pipeline from Part 2 by adding normalizer and ml_classifier\n",
    "pipeline = Pipeline(stages=[pipeline_load, normalizer, ml_classifier])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply different parameters to the *LinearSVC* classifier we will use `ParamGridBuilder`. We will set\n",
    "- maxIter = [10, 15]\n",
    "- regParam = [0.001, 0.01, 0.1]\n",
    "- standardization = [True, False]\n",
    "\n",
    "For tuning, we will use *TrainValidationSplit* because it is less expensive compared to *CrossValidator*. In order to evaluate the performance of the trained classifiers, we will use `MulticlassClassificationEvaluator` with `f1` measure. We will set `trainRatio=0.8` so that 80% of the data to be used for training and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n",
    "\n",
    "# Apply different parameters to the SVC classifier\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(classifier.maxIter, [10, 15]) \\\n",
    "    .addGrid(classifier.regParam, [0.001, 0.01, 0.1]) \\\n",
    "    .addGrid(classifier.standardization, [True, False]) \\\n",
    "    .build()\n",
    "\n",
    "# Initializing evaluator for TrainValidationSplit\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=evaluator,\n",
    "                           trainRatio=0.8) # 80% will be used for training, 20% for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will fit the model with the train set we created and then save the best model for future use in the specified path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvsModel = tvs.fit(train)\n",
    "tvsBestModel = tvsModel.bestModel\n",
    "tvsBestModel.write().overwrite().save(\"/user/Solution/tvsBestModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the best model to transform validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction for validate data\n",
    "validation = tvsBestModel.transform(validate)\n",
    "# Create prediction for test data\n",
    "prediction = tvsBestModel.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the accuray of the models, we will use the MulticlassClassificationEvaluator we initialized before:\n",
    "- Accuracy of validation using the best model is calculated => *evaluator.evaluate(validation)*\n",
    "- Accuracy of prediction using the best model is calculated => *evaluator.evaluate(prediction)*\n",
    "\n",
    "We will also get all the parameters used to create the best model and save all in *output_model.txt* file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeToFile = \"Accuracy of validation using the best model is = \" + str(round(evaluator.evaluate(validation) * 100, 2)) + \"%\"\n",
    "writeToFile += \"\\nAccuracy of prediction using the best model is = \" + str(round(evaluator.evaluate(prediction) * 100, 2)) + \"%\"\n",
    "writeToFile += \"\\n\\nBest model parameters:\"\n",
    "writeToFile += \"\\nMaximum iterations = \" + str(tvsBestModel.stages[-1].getClassifier().getMaxIter())\n",
    "writeToFile += \"\\nRegularization = \" + str(tvsBestModel.stages[-1].getClassifier().getRegParam())\n",
    "writeToFile += \"\\nStandardization = \" + str(tvsBestModel.stages[-1].getClassifier().getStandardization())\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "# convert writeToFile string to a spark dataframe\n",
    "writeFile = spark.createDataFrame([writeToFile], StringType())\n",
    "# saving writeFile dataframe to a file\n",
    "writeFile.coalesce(1).write.format(\"text\").mode('overwrite').save(\"/user/Solution/output_model.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "python",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}