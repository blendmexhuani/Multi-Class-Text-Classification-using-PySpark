{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2) Datasets/DataFrames: Spark ML and Pipelines\n",
    "\n",
    "Convert the review texts to a classic vector space representation with TFIDF-weighted features based on the Spark DataFrame/Dataset API by building a transformation pipeline. The primary goal of this part is the preparation of the pipeline for Part 3 (see Part3.ipynb file).\n",
    "\n",
    "By the help of built-in functions, do the tokenization to unigrams with whitespaces, tabs, digits, and common delimiter characters, casefolding, stopword removal, TF-IDF calculation, and chi square selection (using 4.000 top terms). Write the terms selected this way to a file output_ds.txt.\n",
    "\n",
    "*Commands used in terminal in order to execute this notebook:*\n",
    "- **jupyter nbconvert Part2.ipynb --to script** *(for converting jupyter notebook file to python file in order to execute it via spark-submit)*\n",
    "- **spark-submit --executor-memory 8G --num-executors 4 --total-executor-cores 16 --conf spark.ui.port=5051 Part2.py**\n",
    "\n",
    "For this part, we will first set the configurations and initialize the session and context which we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName(\"Part 2) Datasets/DataFrames: Spark ML and Pipelines\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data\n",
    "\n",
    "We will read the data as a json file by using `spark.read.json` and select *category* and *reviewText* for further analyses. We will also read *stopwords* as text file by using `sc.textFile` and store them as a list of words.\n",
    "\n",
    "An array named *stages* will be initialize to collect all the stages for pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data as json file and select category and reviewText\n",
    "data = spark.read.json(\"/data/reviews_devset.json\").select('category', 'reviewText')\n",
    "# Read the stopwords as a list\n",
    "stopwords = sc.textFile(\"/data/stopwords\").collect()\n",
    "# Initialize stages as an array to collect all the stages for pipeline\n",
    "stages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building stages for pipeline creation\n",
    "\n",
    "We will use *RegexTokenizer* to apply the pattern of delimiters to the reviewText. Then, we will add the `regexTokenizer` to the stages array. The output column of the *RegexTokenizer* is *words*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=\"\\\\d+|\\\\t|\\\\.|\\?|!|,|;|\\\\:|\\(|\\)|\\[|\\]|\\{|\\}|-|\\\"|`|~|#|&|\\*|%|\\$|\\\\\\\\|/|\\\\s+\").setMinTokenLength(2)\n",
    "\n",
    "stages += [regexTokenizer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use *StopWordsRemover* in the `regexTokenizer` output column to filter the words that are is stopwords array. Then, we will add the `remover` to the stages array. The output column of the *StopWordsRemover* is *filtered_words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=regexTokenizer.getOutputCol(), outputCol=\"filtered_words\", stopWords=stopwords, caseSensitive=False)\n",
    "\n",
    "stages += [remover]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use *CountVectorizer* in remover's output column to calculate `tf - term frequecy`*(the number of times that term f appears in document d, while document frequency DF is the number of documents that contains term t)*. Then, we will use the output colum of the *CountVectorizer* as an input column of *IDF* to calculate `tfidf = tf * idf`*(where idf is the inverse document frequency)* that we will need for the chi-square calculation. The output column of the *CountVectorizer* is *tf* and of the *IDF* is *tfidf*.\n",
    "\n",
    "We will add `cv` and `idf` to the stages array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "\n",
    "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"tf\")\n",
    "idf = IDF(inputCol=cv.getOutputCol(), outputCol=\"tfidf\")\n",
    "\n",
    "stages += [cv, idf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use `category` column as a label column in the *ChiSqSelector*, it should be indexed first. We will use *StringIndexer* for that then we will add the `indexer` to the stages array. The output column of the *StringIndexer* is *label*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "\n",
    "stages += [indexer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *ChiSqSelector* will be used to calculate chi-square of the features selected by *tfidf*. We will select top 4000 features and output the results in the *selectedFeatures* column. Then, we will add the `selector` to the stages array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "\n",
    "selector = ChiSqSelector(numTopFeatures=4000, featuresCol=\"tfidf\", outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "\n",
    "stages += [selector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up pipeline and fitting the model with it\n",
    "\n",
    "After creating all the stages, we can create now the Pipeline model and fit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the model we fitted by using `model.write().overwrite().save(path)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"/Solution/pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting top 4000 words from the model and saving them into a file\n",
    "\n",
    "We will take the vocabulary from the pipeline's *CountVectorizerModel* and store it in an array called `cvModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizerModel\n",
    "\n",
    "# get CountVectorizerModel from model stages\n",
    "vectorizers = [s for s in model.stages if isinstance(s, CountVectorizerModel)]\n",
    "# get vocabulary from vectorizers\n",
    "cvModel = [v.vocabulary for v in vectorizers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will transform the model we created in order to get chi-square calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiSqModel = model.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the terms, we first need to get all the indices from *selectedFeatures* column and store them in a different column. After that, we will iterate for each indice and find the word in the vocabulary that corresponds to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# getting the list of indices of the selectedFeatures column\n",
    "feature_extract_keys = udf(lambda vector: vector.indices.tolist(), ArrayType(IntegerType()))\n",
    "\n",
    "# getting the list of words that correspond to the indices we extracted in the chiSquareKeys column\n",
    "def indices_to_terms(vocabulary):\n",
    "    def indices_to_terms(xs):\n",
    "        return [vocabulary[int(x)] for x in xs]\n",
    "    return udf(indices_to_terms, ArrayType(StringType()))\n",
    "\n",
    "chiSqModel = chiSqModel.withColumn(\"chiSquareKeys\", feature_extract_keys(col(\"selectedFeatures\")))\n",
    "chiSqModel = chiSqModel.withColumn(\"terms\", indices_to_terms(list(list(cvModel)[0]))(\"chiSquareKeys\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will select the *terms* column we created and save it in an array. Then, we will concatenate all the words in that array, by space and also, by sorting them ascending, and save them in a string variable called `wordsToFile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "import numpy as np\n",
    "\n",
    "# selecting terms column as a list from chiSqModel\n",
    "termsList = chiSqModel.select(collect_list('terms')).first()[0]\n",
    "# joining the list of words in termsList by space-separating them\n",
    "wordsToFile = \" \".join(sorted(set(np.concatenate(termsList).ravel())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert `wordsToFile` variable into a spark dataframe in order to save it into a text file called *output_ds.txt*. \n",
    "\n",
    "We will use `.coalesce(1).write.format(\"text\").mode('overwrite').save(path)` in order to overwrite the file if exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert wordsToFile string to a spark dataframe\n",
    "one_line_words = spark.createDataFrame([wordsToFile], StringType())\n",
    "# saving one_line_words dataframe to a file\n",
    "one_line_words.coalesce(1).write.format(\"text\").mode('overwrite').save(\"/Solution/output_ds.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "python",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}