{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1) RDDs\n",
    "\n",
    "Calculate chi-square values using RDDs and transformations. Write the output to a file output_rdd.txt.\n",
    "\n",
    "For this part, we will first set the configurations and initialize the context which we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"Part 1) RDDs\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data\n",
    "\n",
    "We will use spark context sc - `sc.textFile` to read the file and then `json.loads` to load as json and select *category* and *reviewText* for further analysis. Also I will load stopwords with `sc.textFile` too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# create rdd\n",
    "documents = sc.textFile(\"hdfs:///user/data/reviews_devset.json\")\n",
    "# load as json\n",
    "dataset = documents.map(json.loads)\n",
    "# select only reviewteext\n",
    "rdd_data = dataset.map(lambda e: (e['category'], e['reviewText']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read stopwords as a list\n",
    "stopwords = sc.textFile(\"/user/data/stopwords\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "#### Tokenization, case folding and removing stop words\n",
    "\n",
    "In this step we created a function called `tokenization_casefolding_stopwords` that will be called in `flatMap` function for every row.\n",
    "\n",
    "We will `import re` because we need it in this tokenization process.\n",
    "For each line the function will set all the line to lower case (for case folding) and than after substituting the delimiters like tabs, digits, and common delimiter characters, we will split the line by whitespaces and get only the unique words.\n",
    "\n",
    "Each word will be checked if it is a stopword before returning. This function will return three different outputs:\n",
    "\n",
    "- `(categoryCount~category, 1)` will count categories\n",
    "- `(wordCount~word, 1)` will count the occurance of unique words in reviews\n",
    "- `((category,word), 1)` will count the number of reviews in category which contain the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "delimiter = \"\\\\d+|\\\\t|\\\\.|\\?|!|,|;|\\\\:|\\(|\\)|\\[|\\]|\\{|\\}|-|\\\"|`|~|#|&|\\*|%|\\$|\\\\\\\\|/\"\n",
    "def tokenization_casefolding_stopwords(row):\n",
    "    # get the text from the row entry and lower case reviewText\n",
    "    category = row[0]\n",
    "    reviewText = row[1].lower()\n",
    "    # remove unwanted chars\n",
    "    reviewText = re.sub(delimiter, \" \",reviewText)\n",
    "    # split by space and get only the unique words\n",
    "    words = reviewText.split(\" \")\n",
    "    unique_words = list(set(words))\n",
    "    # ((category,word), 1) tuple\n",
    "    categoryWord = map(lambda x: (category + \",\" + x, 1), filter(lambda x: len(x) > 1 and x not in stopwords, unique_words))\n",
    "    # (word, 1) tuple\n",
    "    wordCount = map(lambda x: (\"wordCount~\"+ x, 1), filter(lambda x: len(x) > 1 and x not in stopwords, unique_words))\n",
    "    # return ((category,word), 1) tuples, (word, 1) tuples and (category, 1) tuples\n",
    "    return (list(categoryWord) + list(wordCount) + list([(\"categoryCount~\" + category, 1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the number of rows in the dataset we use `rdd_data.count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = rdd_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will call the function `tokenization_casefolding_stopwords` for every row and after we will `reduceByKey` the output of the mapper by summing the values.\n",
    "\n",
    "After the mapped_rdd is done, we will get all the `categoryWord~` values and store them in an *categoryWord* rdd.\n",
    "To store `categoryCount~` and `wordCount~` values, we will use `collectAsMap` action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_rdd = rdd_data.flatMap(lambda row: tokenization_casefolding_stopwords(row)).reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryWord = mapped_rdd.filter(lambda x: \"wordCount~\" not in x[0] and \"categoryCount~\" not in x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultAsMap = mapped_rdd.filter(lambda x: \"wordCount~\" in x[0] or \"categoryCount~\" in x[0]).collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "In this step we will calculate chi-square values for each `(category,word)` tuple. To achieve this we created a function called `chi_square_calc`. \n",
    "\n",
    "This function will take each tuple and calculate the chi-square by also involving `categories` and `words` that we have in `resultAsMap`. The function will output: `(category, word:chi-square)` tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_calc(row):\n",
    "    category = row[0].split(\",\")[0]\n",
    "    word = row[0].split(\",\")[1]\n",
    "    cCount = resultAsMap[\"categoryCount~\" + category]\n",
    "    wCount = resultAsMap[\"wordCount~\" + word]\n",
    "    A = row[1]\n",
    "    B = wCount - A\n",
    "    C = cCount - A\n",
    "    D = N - A - B - C\n",
    "    chi_square = N * ((A * D) - (B * C)) * ((A * D) - (B * C)) / ((A + B) * (A + C) * (B + D) * (C + D))\n",
    "    return (category, word + \":\" + str(chi_square))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating the chi-square values for each `(category,word)` tuple, we will `reduceByKey()` the result in order to group them by category and sort them. Then we will use `mapValues()` to sort the chi-square values in the descending order, but we first need to split the values we concatenated in the `reduceByKey()`, and also to take top 200 values for each category.\n",
    "\n",
    "In the end we will join *categories* with their `(word:chi-square)` tuples by space-separating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_values(values):\n",
    "    return values[0] + \" \" + \" \".join(values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = categoryWord.map(lambda row: chi_square_calc(row)) \\\n",
    "    .reduceByKey(lambda x, y: x + \" \" + y) \\\n",
    "    .mapValues(lambda values: sorted(values.split(\" \"), key=lambda x: float(x.split(\":\")[1]), reverse=True)[:200]) \\\n",
    "    .sortByKey() \\\n",
    "    .map(lambda x: join_values(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "In this step we will create a line with all unique words, sorted ascending, that are in the `results` rdd from previous step by first storing them in an array and then joining them together to create by using `join()` function.\n",
    "\n",
    "After that we will join `results` rdd with the `one_line_rdd` we created and output the results in a text file by using `.saveAsTextFile()`. Before saving, we will check if the file exists in order to delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_array = sorted(results.flatMap(lambda x: x.split(\" \")[1: ]).map(lambda x: x.split(\":\")[0] + \" \").distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_line_words = \"\".join(words_array).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_line_rdd = sc.parallelize([one_line_words])\n",
    "output = sc.union([results, one_line_rdd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToSave = \"/user/Solution/output_rdd.txt\"\n",
    "\n",
    "try:\n",
    "    import subprocess\n",
    "    subprocess.call([\"hadoop\", \"fs\", \"-rm\", \"-r\", \"-skipTrash\", pathToSave])\n",
    "except IOError:\n",
    "    \"\"\n",
    "\n",
    "output.saveAsTextFile(pathToSave)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "python",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}